{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ01up0p7wOJ"
      },
      "source": [
        "## ðŸš… liteLLM Quick Start Demo\n",
        "### TLDR: Call 50+ LLM APIs using chatGPT Input/Output format\n",
        "https://github.com/BerriAI/litellm\n",
        "\n",
        "liteLLM is package to simplify calling **OpenAI, Azure, Llama2, Cohere, Anthropic, Huggingface API Endpoints**. LiteLLM manages\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RZtzCnQS7rW-"
      },
      "source": [
        "## Installation and setting Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsrN5W-N7L8d"
      },
      "outputs": [],
      "source": [
        "!pip install litellm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ArrWyG5b7QAG"
      },
      "outputs": [],
      "source": [
        "from litellm import completion\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bbhJRt34_NJ1"
      },
      "source": [
        "## Set your API keys\n",
        "- liteLLM reads your .env, env variables or key manager for Auth\n",
        "\n",
        "Set keys for the models you want to use below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"length\",\n",
            "      \"index\": 1,\n",
            "      \"message\": {\n",
            "        \"content\": \" interested in\\nI am interested in learning more about your LLM program and the specific fields or topics it entails.\",\n",
            "        \"role\": \"assistant\"\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"id\": \"chatcmpl-1894500f-415e-43b1-b623-1f95ad36571d\",\n",
            "  \"created\": 1698323703.212266,\n",
            "  \"model\": \"j2-ultra\",\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 6,\n",
            "    \"completion_tokens\": 23,\n",
            "    \"total_tokens\": 29\n",
            "  }\n",
            "}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import litellm\n",
        "import os\n",
        "os.environ['AI21_API_KEY'] = \"QZDOVYAGdnRZK3241KAlJkxDo9OX9t7s\"\n",
        "\n",
        "response = litellm.completion(\n",
        "    model=\"ai21/j2-ultra\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"hi what llm are you\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-h8Ga5cR7SvV"
      },
      "outputs": [],
      "source": [
        "# Only set keys for the LLMs you want to use\n",
        "os.environ['OPENAI_API_KEY'] = \"sk-bs4GXoj6A57MtlSK74B9C8Cc20224e3090489e6bB140D8C8\" #@param\n",
        "os.environ['OPENAI_API_BASE'] = \"https://api.api-fly.com/v1\" #@param\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"\" #@param\n",
        "os.environ[\"REPLICATE_API_KEY\"] = \"\" #@param\n",
        "os.environ[\"COHERE_API_KEY\"] = \"\" #@param\n",
        "os.environ[\"AZURE_API_BASE\"] = \"\" #@param\n",
        "os.environ[\"AZURE_API_VERSION\"] = \"\" #@param\n",
        "os.environ[\"AZURE_API_KEY\"] = \"\" #@param"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fhqpKv6L8fBj"
      },
      "source": [
        "## Call chatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "speIkoX_8db4",
        "outputId": "331a6c65-f121-4e65-e121-bf8aaad05d9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-8DwCtU3rdegLlbyUL7twiC3GzExx7 at 0x124f08770> JSON: {\n",
              "  \"id\": \"chatcmpl-8DwCtU3rdegLlbyUL7twiC3GzExx7\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"created\": 1698332703,\n",
              "  \"model\": \"gpt-3.5-turbo-0613\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"role\": \"assistant\",\n",
              "        \"content\": \"I'm sorry, but as an AI text-based model, I don't have real-time data. However, you can easily check the weather in San Francisco by searching for \\\"weather in San Francisco\\\" on a search engine or using a weather website or app.\"\n",
              "      },\n",
              "      \"finish_reason\": \"stop\"\n",
              "    }\n",
              "  ],\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 13,\n",
              "    \"completion_tokens\": 52,\n",
              "    \"total_tokens\": 65\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "completion(model=\"gpt-3.5-turbo\", messages=[{ \"content\": \"what's the weather in SF\",\"role\": \"user\"}])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3jV1Uxv8zNo"
      },
      "source": [
        "## Call Claude-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8yTWYzY8m9S",
        "outputId": "8b6dd32d-f9bf-4e89-886d-47cb8020f025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n"
          ]
        },
        {
          "ename": "APIError",
          "evalue": "Missing Anthropic API Key - A call is being made to anthropic but no key is set either in the environment variables or via params",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/main.py:604\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, deployment_id, request_timeout, api_base, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m api_base \u001b[39m=\u001b[39m (\n\u001b[1;32m    599\u001b[0m     api_base\n\u001b[1;32m    600\u001b[0m     \u001b[39mor\u001b[39;00m litellm\u001b[39m.\u001b[39mapi_base\n\u001b[1;32m    601\u001b[0m     \u001b[39mor\u001b[39;00m get_secret(\u001b[39m\"\u001b[39m\u001b[39mANTHROPIC_API_BASE\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    602\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhttps://api.anthropic.com/v1/complete\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m )\n\u001b[0;32m--> 604\u001b[0m model_response \u001b[39m=\u001b[39m anthropic\u001b[39m.\u001b[39;49mcompletion(\n\u001b[1;32m    605\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    606\u001b[0m     messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m    607\u001b[0m     api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[1;32m    608\u001b[0m     custom_prompt_dict\u001b[39m=\u001b[39;49mlitellm\u001b[39m.\u001b[39;49mcustom_prompt_dict,\n\u001b[1;32m    609\u001b[0m     model_response\u001b[39m=\u001b[39;49mmodel_response,\n\u001b[1;32m    610\u001b[0m     print_verbose\u001b[39m=\u001b[39;49mprint_verbose,\n\u001b[1;32m    611\u001b[0m     optional_params\u001b[39m=\u001b[39;49moptional_params,\n\u001b[1;32m    612\u001b[0m     litellm_params\u001b[39m=\u001b[39;49mlitellm_params,\n\u001b[1;32m    613\u001b[0m     logger_fn\u001b[39m=\u001b[39;49mlogger_fn,\n\u001b[1;32m    614\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding, \u001b[39m# for calculating input/output tokens\u001b[39;49;00m\n\u001b[1;32m    615\u001b[0m     api_key\u001b[39m=\u001b[39;49manthropic_key,\n\u001b[1;32m    616\u001b[0m     logging_obj\u001b[39m=\u001b[39;49mlogging, \n\u001b[1;32m    617\u001b[0m )\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m optional_params \u001b[39mand\u001b[39;00m optional_params[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[39m# don't try to access stream object,\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/llms/anthropic.py:85\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, api_base, custom_prompt_dict, model_response, print_verbose, encoding, api_key, logging_obj, optional_params, litellm_params, logger_fn)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompletion\u001b[39m(\n\u001b[1;32m     72\u001b[0m     model: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     73\u001b[0m     messages: \u001b[39mlist\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     logger_fn\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m ):\n\u001b[0;32m---> 85\u001b[0m     headers \u001b[39m=\u001b[39m validate_environment(api_key)\n\u001b[1;32m     86\u001b[0m     \u001b[39mif\u001b[39;00m model \u001b[39min\u001b[39;00m custom_prompt_dict:\n\u001b[1;32m     87\u001b[0m             \u001b[39m# check if the model has a registered custom prompt\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/llms/anthropic.py:60\u001b[0m, in \u001b[0;36mvalidate_environment\u001b[0;34m(api_key)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m api_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     61\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMissing Anthropic API Key - A call is being made to anthropic but no key is set either in the environment variables or via params\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m headers \u001b[39m=\u001b[39m {\n\u001b[1;32m     64\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39maccept\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39manthropic-version\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m2023-06-01\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     66\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcontent-type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mx-api-key\u001b[39m\u001b[39m\"\u001b[39m: api_key,\n\u001b[1;32m     68\u001b[0m }\n",
            "\u001b[0;31mValueError\u001b[0m: Missing Anthropic API Key - A call is being made to anthropic but no key is set either in the environment variables or via params",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/private/var/www/litellm/cookbook/liteLLM_Getting_Started.ipynb å•å…ƒæ ¼ 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/private/var/www/litellm/cookbook/liteLLM_Getting_Started.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m completion(model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclaude-2\u001b[39;49m\u001b[39m\"\u001b[39;49m, messages\u001b[39m=\u001b[39;49m[{ \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mwhat\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms the weather in SF\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m}])\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/utils.py:830\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    827\u001b[0m         liteDebuggerClient \u001b[39mand\u001b[39;00m liteDebuggerClient\u001b[39m.\u001b[39mdashboard_url \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    828\u001b[0m     ):  \u001b[39m# make it easy to get to the debugger logs if you've initialized it\u001b[39;00m\n\u001b[1;32m    829\u001b[0m         e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Check the log in your dashboard - \u001b[39m\u001b[39m{\u001b[39;00mliteDebuggerClient\u001b[39m.\u001b[39mdashboard_url\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 830\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/utils.py:789\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[39mreturn\u001b[39;00m cached_result\n\u001b[1;32m    788\u001b[0m \u001b[39m# MODEL CALL\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m result \u001b[39m=\u001b[39m original_function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    790\u001b[0m end_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m    791\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    792\u001b[0m     \u001b[39m# TODO: Add to cache for streaming\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/timeout.py:53\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     local_timeout_duration \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     result \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mlocal_timeout_duration)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m futures\u001b[39m.\u001b[39mTimeoutError:\n\u001b[1;32m     55\u001b[0m     thread\u001b[39m.\u001b[39mstop_loop()\n",
            "File \u001b[0;32m/usr/local/Cellar/python@3.9/3.9.6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:445\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    444\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 445\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    446\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
            "File \u001b[0;32m/usr/local/Cellar/python@3.9/3.9.6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/timeout.py:42\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorator.<locals>.wrapper.<locals>.async_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39masync_func\u001b[39m():\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/main.py:1265\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, deployment_id, request_timeout, api_base, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m   1263\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1264\u001b[0m     \u001b[39m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 1265\u001b[0m     \u001b[39mraise\u001b[39;00m exception_type(\n\u001b[1;32m   1266\u001b[0m         model\u001b[39m=\u001b[39;49mmodel, custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider, original_exception\u001b[39m=\u001b[39;49me, completion_kwargs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1267\u001b[0m     )\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/utils.py:3351\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   3349\u001b[0m \u001b[39m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[39;00m\n\u001b[1;32m   3350\u001b[0m \u001b[39mif\u001b[39;00m exception_mapping_worked:\n\u001b[0;32m-> 3351\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   3352\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3353\u001b[0m     \u001b[39mraise\u001b[39;00m original_exception\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/utils.py:3333\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   3327\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidRequestError(\n\u001b[1;32m   3328\u001b[0m             message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOpenAIException: This can happen due to missing AZURE_API_VERSION: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(original_exception)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3329\u001b[0m             model\u001b[39m=\u001b[39mmodel, \n\u001b[1;32m   3330\u001b[0m             llm_provider\u001b[39m=\u001b[39mcustom_llm_provider\n\u001b[1;32m   3331\u001b[0m         )\n\u001b[1;32m   3332\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3333\u001b[0m         \u001b[39mraise\u001b[39;00m APIError(status_code\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(original_exception), llm_provider\u001b[39m=\u001b[39mcustom_llm_provider, model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m   3334\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   3335\u001b[0m     \u001b[39m# LOGGING\u001b[39;00m\n\u001b[1;32m   3336\u001b[0m     exception_logging(\n\u001b[1;32m   3337\u001b[0m         logger_fn\u001b[39m=\u001b[39muser_logger_fn,\n\u001b[1;32m   3338\u001b[0m         additional_args\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3342\u001b[0m         exception\u001b[39m=\u001b[39me,\n\u001b[1;32m   3343\u001b[0m     )\n",
            "\u001b[0;31mAPIError\u001b[0m: Missing Anthropic API Key - A call is being made to anthropic but no key is set either in the environment variables or via params"
          ]
        }
      ],
      "source": [
        "completion(model=\"claude-2\", messages=[{ \"content\": \"what's the weather in SF\",\"role\": \"user\"}])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yu0LPDmW9PJa"
      },
      "source": [
        "## Call llama2 on replicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GWV5mtO9Jbu",
        "outputId": "38538825-b271-406d-a437-f5cf0eb7e548"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<ModelResponse chat.completion id=chatcmpl-3151c2eb-b26f-4c96-89b5-ed1746b219e0 at 0x138b87e50> JSON: {\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \" I'm happy to help! However, I must point out that the question \\\"what's the weather in SF\\\" doesn't make sense as \\\"SF\\\" could refer to multiple locations. Could you please clarify which location you are referring to? San Francisco, California or Sioux Falls, South Dakota? Once I have more context, I would be happy to provide you with accurate and reliable information.\",\n",
              "        \"role\": \"assistant\",\n",
              "        \"logprobs\": null\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"id\": \"chatcmpl-3151c2eb-b26f-4c96-89b5-ed1746b219e0\",\n",
              "  \"created\": 1695490237.714101,\n",
              "  \"response_ms\": 12109.565,\n",
              "  \"model\": \"replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1\",\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 6,\n",
              "    \"completion_tokens\": 78,\n",
              "    \"total_tokens\": 84\n",
              "  },\n",
              "  \"ended\": 1695490249.821266\n",
              "}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = \"replicate/llama-2-70b-chat:2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1\"\n",
        "completion(model=model, messages=[{ \"content\": \"what's the weather in SF\",\"role\": \"user\"}])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HXdj5SEe9iLK"
      },
      "source": [
        "## Call Command-Nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaUq2xIx9fhr",
        "outputId": "55fe6f52-b58b-4729-948a-74dac4b431b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<ModelResponse chat.completion id=chatcmpl-dc0d8ead-071d-486c-a111-78975b38794b at 0x1389725e0> JSON: {\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \" As an AI model I don't have access to real-time data, so I can't tell\",\n",
              "        \"role\": \"assistant\",\n",
              "        \"logprobs\": null\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"id\": \"chatcmpl-dc0d8ead-071d-486c-a111-78975b38794b\",\n",
              "  \"created\": 1695490235.936903,\n",
              "  \"response_ms\": 1022.6759999999999,\n",
              "  \"model\": \"command-nightly\",\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 6,\n",
              "    \"completion_tokens\": 19,\n",
              "    \"total_tokens\": 25\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "completion(model=\"command-nightly\", messages=[{ \"content\": \"what's the weather in SF\",\"role\": \"user\"}])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1g9hSgsL9soJ"
      },
      "source": [
        "## Call Azure OpenAI"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For azure openai calls ensure to add the `azure/` prefix to `model`. If your deployment-id is `chatgpt-test` set `model` = `azure/chatgpt-test`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvLjR-PF-lt0",
        "outputId": "deff2db3-b003-48cd-ea62-c03a68a4464a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-820kZyCwbNvZATiLkNmXmpxxzvTKO at 0x138b84ae0> JSON: {\n",
              "  \"id\": \"chatcmpl-820kZyCwbNvZATiLkNmXmpxxzvTKO\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"created\": 1695490231,\n",
              "  \"model\": \"gpt-35-turbo\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"index\": 0,\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"message\": {\n",
              "        \"role\": \"assistant\",\n",
              "        \"content\": \"Sorry, as an AI language model, I don't have real-time information. Please check your preferred weather website or app for the latest weather updates of San Francisco.\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 33,\n",
              "    \"prompt_tokens\": 14,\n",
              "    \"total_tokens\": 47\n",
              "  },\n",
              "  \"response_ms\": 1499.529\n",
              "}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "completion(model=\"azure/chatgpt-v-2\", messages=[{ \"content\": \"what's the weather in SF\",\"role\": \"user\"}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
