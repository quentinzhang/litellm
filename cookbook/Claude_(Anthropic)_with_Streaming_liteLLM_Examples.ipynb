{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwuaylskLxFu",
        "outputId": "d684d6a3-32fe-4beb-c378-c39134bcf8cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting litellm==0.1.363\n",
            "  Downloading litellm-0.1.363-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: openai<0.28.0,>=0.27.8 in /usr/local/lib/python3.10/dist-packages (from litellm==0.1.363) (0.27.8)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from litellm==0.1.363) (1.0.0)\n",
            "Requirement already satisfied: tiktoken<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from litellm==0.1.363) (0.4.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.8->litellm==0.1.363) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.8->litellm==0.1.363) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.8->litellm==0.1.363) (3.8.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.5.0,>=0.4.0->litellm==0.1.363) (2022.10.31)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->litellm==0.1.363) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->litellm==0.1.363) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->litellm==0.1.363) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->litellm==0.1.363) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->litellm==0.1.363) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->litellm==0.1.363) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->litellm==0.1.363) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->litellm==0.1.363) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->litellm==0.1.363) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->litellm==0.1.363) (1.3.1)\n",
            "Installing collected packages: litellm\n",
            "  Attempting uninstall: litellm\n",
            "    Found existing installation: litellm 0.1.362\n",
            "    Uninstalling litellm-0.1.362:\n",
            "      Successfully uninstalled litellm-0.1.362\n",
            "Successfully installed litellm-0.1.363\n"
          ]
        }
      ],
      "source": [
        "!pip install litellm==\"0.1.363\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W216G__XL19Q"
      },
      "outputs": [],
      "source": [
        "# @title Import litellm & Set env variables\n",
        "import litellm\n",
        "import os\n",
        "litellm.set_verbose=True\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"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\" #@param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff1lKwUMMLJj",
        "outputId": "bfddf6f8-36d4-45e5-92dc-349083fa41b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
            "\n",
            "üëâ view error logs:\n",
            "\u001b[91m\u001b[4mhttps://logs.litellm.ai/?data=%7B%22EXCEPTION%22%3A%20%22LLM%20Provider%20NOT%20provided.%20Pass%20in%20the%20LLM%20provider%20you%20are%20trying%20to%20call.%20E.g.%20For%20%27Huggingface%27%20inference%20endpoints%20pass%20in%20%60completion%28model%3D%27huggingface/claude-instant-1.2%27%2C..%29%60%20Learn%20more%3A%20https%3A//docs.litellm.ai/docs/providers%22%2C%20%22KWARGS%22%3A%20%7B%22model%22%3A%20%22claude-instant-1.2%22%2C%20%22messages%22%3A%20%22%5B%7B%27role%27%3A%20%27system%27%2C%20%27content%27%3A%20%27You%20are%20a%20helpful%20assistant.%27%7D%2C%20%7B%27role%27%3A%20%27user%27%2C%20%27content%27%3A%20%27Who%20won%20the%20world%20series%20in%202020%3F%27%7D%5D%22%2C%20%22functions%22%3A%20%22%5B%5D%22%2C%20%22function_call%22%3A%20%22%22%2C%20%22temperature%22%3A%20%22None%22%2C%20%22top_p%22%3A%20%22None%22%2C%20%22n%22%3A%20%22None%22%2C%20%22stream%22%3A%20%22None%22%2C%20%22stop%22%3A%20%22None%22%2C%20%22max_tokens%22%3A%20%22None%22%2C%20%22presence_penalty%22%3A%20%22None%22%2C%20%22frequency_penalty%22%3A%20%22None%22%2C%20%22logit_bias%22%3A%20%22%7B%7D%22%2C%20%22user%22%3A%20%22%22%2C%20%22deployment_id%22%3A%20%22None%22%2C%20%22request_timeout%22%3A%20%22None%22%2C%20%22kwargs%22%3A%20%22%7B%27litellm_call_id%27%3A%20%27789848be-ae95-4533-8be4-a8bd1fe3757a%27%2C%20%27litellm_logging_obj%27%3A%20%3Clitellm.utils.Logging%20object%20at%200x112ba1a00%3E%7D%22%7D%7D\u001b[0m\n",
            "LiteLLM: Logging Details: logger_fn - None | callable(logger_fn) - False\n",
            "LiteLLM: Logging Details LiteLLM-Failure Call\n",
            "LiteLLM: self.failure_callback: []\n"
          ]
        },
        {
          "ename": "APIError",
          "evalue": "LLM Provider NOT provided. Pass in the LLM provider you are trying to call. E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/claude-instant-1.2',..)` Learn more: https://docs.litellm.ai/docs/providers",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/main.py:248\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, deployment_id, request_timeout, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m         custom_llm_provider\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mazure\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 248\u001b[0m model, custom_llm_provider \u001b[39m=\u001b[39m get_llm_provider(model\u001b[39m=\u001b[39;49mmodel, custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider)\n\u001b[1;32m    249\u001b[0m model_api_key \u001b[39m=\u001b[39m get_api_key(llm_provider\u001b[39m=\u001b[39mcustom_llm_provider, dynamic_api_key\u001b[39m=\u001b[39mapi_key) \u001b[39m# get the api key from the environment if required for the model\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/utils.py:1398\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider)\u001b[0m\n\u001b[1;32m   1397\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e: \n\u001b[0;32m-> 1398\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/utils.py:1395\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider)\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \u001b[39mprint\u001b[39m()\n\u001b[0;32m-> 1395\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. E.g. For \u001b[39m\u001b[39m'\u001b[39m\u001b[39mHuggingface\u001b[39m\u001b[39m'\u001b[39m\u001b[39m inference endpoints pass in `completion(model=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhuggingface/\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1396\u001b[0m \u001b[39mreturn\u001b[39;00m model, custom_llm_provider\n",
            "\u001b[0;31mValueError\u001b[0m: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/claude-instant-1.2',..)` Learn more: https://docs.litellm.ai/docs/providers",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/private/var/www/litellm/cookbook/Claude_(Anthropic)_with_Streaming_liteLLM_Examples.ipynb ÂçïÂÖÉÊ†º 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/private/var/www/litellm/cookbook/Claude_%28Anthropic%29_with_Streaming_liteLLM_Examples.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# @title Request Claude Instant-1 and Claude-2\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/private/var/www/litellm/cookbook/Claude_%28Anthropic%29_with_Streaming_liteLLM_Examples.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m messages \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell:/private/var/www/litellm/cookbook/Claude_%28Anthropic%29_with_Streaming_liteLLM_Examples.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mYou are a helpful assistant.\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m      <a href='vscode-notebook-cell:/private/var/www/litellm/cookbook/Claude_%28Anthropic%29_with_Streaming_liteLLM_Examples.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mWho won the world series in 2020?\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m      <a href='vscode-notebook-cell:/private/var/www/litellm/cookbook/Claude_%28Anthropic%29_with_Streaming_liteLLM_Examples.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   ]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/private/var/www/litellm/cookbook/Claude_%28Anthropic%29_with_Streaming_liteLLM_Examples.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m result \u001b[39m=\u001b[39m litellm\u001b[39m.\u001b[39;49mcompletion(\u001b[39m'\u001b[39;49m\u001b[39mclaude-instant-1.2\u001b[39;49m\u001b[39m'\u001b[39;49m, messages)\n\u001b[1;32m      <a href='vscode-notebook-cell:/private/var/www/litellm/cookbook/Claude_%28Anthropic%29_with_Streaming_liteLLM_Examples.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m Result from claude-instant-1.2\u001b[39m\u001b[39m\"\u001b[39m, result)\n\u001b[1;32m      <a href='vscode-notebook-cell:/private/var/www/litellm/cookbook/Claude_%28Anthropic%29_with_Streaming_liteLLM_Examples.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m result \u001b[39m=\u001b[39m litellm\u001b[39m.\u001b[39mcompletion(\u001b[39m'\u001b[39m\u001b[39mclaude-2\u001b[39m\u001b[39m'\u001b[39m, messages, max_tokens\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/utils.py:726\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    723\u001b[0m         liteDebuggerClient \u001b[39mand\u001b[39;00m liteDebuggerClient\u001b[39m.\u001b[39mdashboard_url \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     ):  \u001b[39m# make it easy to get to the debugger logs if you've initialized it\u001b[39;00m\n\u001b[1;32m    725\u001b[0m         e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Check the log in your dashboard - \u001b[39m\u001b[39m{\u001b[39;00mliteDebuggerClient\u001b[39m.\u001b[39mdashboard_url\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 726\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/utils.py:685\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[39mreturn\u001b[39;00m cached_result\n\u001b[1;32m    684\u001b[0m \u001b[39m# MODEL CALL\u001b[39;00m\n\u001b[0;32m--> 685\u001b[0m result \u001b[39m=\u001b[39m original_function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    686\u001b[0m end_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m    687\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[39m# TODO: Add to cache for streaming\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/timeout.py:53\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     local_timeout_duration \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     result \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mlocal_timeout_duration)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m futures\u001b[39m.\u001b[39mTimeoutError:\n\u001b[1;32m     55\u001b[0m     thread\u001b[39m.\u001b[39mstop_loop()\n",
            "File \u001b[0;32m/usr/local/Cellar/python@3.9/3.9.6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:445\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    444\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 445\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    446\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
            "File \u001b[0;32m/usr/local/Cellar/python@3.9/3.9.6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/timeout.py:42\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorator.<locals>.wrapper.<locals>.async_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39masync_func\u001b[39m():\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/main.py:1121\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, functions, function_call, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, deployment_id, request_timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m   1119\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1120\u001b[0m     \u001b[39m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 1121\u001b[0m     \u001b[39mraise\u001b[39;00m exception_type(\n\u001b[1;32m   1122\u001b[0m         model\u001b[39m=\u001b[39;49mmodel, custom_llm_provider\u001b[39m=\u001b[39;49mcustom_llm_provider, original_exception\u001b[39m=\u001b[39;49me, completion_kwargs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1123\u001b[0m     )\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/utils.py:2891\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[39m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[39;00m\n\u001b[1;32m   2890\u001b[0m \u001b[39mif\u001b[39;00m exception_mapping_worked:\n\u001b[0;32m-> 2891\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   2892\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2893\u001b[0m     \u001b[39mraise\u001b[39;00m original_exception\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/litellm/utils.py:2873\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs)\u001b[0m\n\u001b[1;32m   2871\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImport error - trying to use async for ollama. import async_generator failed. Try \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpip install async_generator\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2872\u001b[0m     exception_mapping_worked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 2873\u001b[0m     \u001b[39mraise\u001b[39;00m APIError(status_code\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(original_exception), llm_provider\u001b[39m=\u001b[39mcustom_llm_provider, model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m   2874\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   2875\u001b[0m     \u001b[39m# LOGGING\u001b[39;00m\n\u001b[1;32m   2876\u001b[0m     exception_logging(\n\u001b[1;32m   2877\u001b[0m         logger_fn\u001b[39m=\u001b[39muser_logger_fn,\n\u001b[1;32m   2878\u001b[0m         additional_args\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2882\u001b[0m         exception\u001b[39m=\u001b[39me,\n\u001b[1;32m   2883\u001b[0m     )\n",
            "\u001b[0;31mAPIError\u001b[0m: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/claude-instant-1.2',..)` Learn more: https://docs.litellm.ai/docs/providers"
          ]
        }
      ],
      "source": [
        "# @title Request Claude Instant-1 and Claude-2\n",
        "messages = [\n",
        "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "  {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
        "  ]\n",
        "\n",
        "result = litellm.completion('claude-instant-1.2', messages)\n",
        "print(\"\\n\\n Result from claude-instant-1.2\", result)\n",
        "result = litellm.completion('claude-2', messages, max_tokens=5, temperature=0.2)\n",
        "print(\"\\n\\n Result from claude-2\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06hWKnNQMrV-",
        "outputId": "7fdec0eb-d4a9-4882-f9c4-987ff9a31114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Here\n",
            "'s\n",
            " a\n",
            " quick\n",
            " overview\n",
            " of\n",
            " how\n",
            " a\n",
            " court\n",
            " case\n",
            " can\n",
            " reach\n",
            " the\n",
            " U\n",
            ".\n",
            "S\n",
            ".\n",
            " Supreme\n",
            " Court\n",
            ":\n",
            "\n",
            "\n",
            "-\n",
            " The\n",
            " case\n",
            " must\n",
            " first\n",
            " be\n",
            " heard\n",
            " in\n",
            " a\n",
            " lower\n",
            " trial\n",
            " court\n",
            " (\n",
            "either\n",
            " a\n",
            " state\n",
            " court\n",
            " or\n",
            " federal\n",
            " district\n",
            " court\n",
            ").\n",
            " The\n",
            " trial\n",
            " court\n",
            " makes\n",
            " initial\n",
            " r\n",
            "ulings\n",
            " and\n",
            " produces\n",
            " a\n",
            " record\n",
            " of\n",
            " the\n",
            " case\n",
            ".\n",
            "\n",
            "\n",
            "-\n",
            " The\n",
            " losing\n",
            " party\n",
            " can\n",
            " appeal\n",
            " the\n",
            " decision\n",
            " to\n",
            " an\n",
            " appeals\n",
            " court\n",
            " (\n",
            "a\n",
            " state\n",
            " appeals\n",
            " court\n",
            " for\n",
            " state\n",
            " cases\n",
            ",\n",
            " or\n",
            " a\n",
            " federal\n",
            " circuit\n",
            " court\n",
            " for\n",
            " federal\n",
            " cases\n",
            ").\n",
            " The\n",
            " appeals\n",
            " court\n",
            " reviews\n",
            " the\n",
            " trial\n",
            " court\n",
            "'s\n",
            " r\n",
            "ulings\n",
            " and\n",
            " can\n",
            " affirm\n",
            ",\n",
            " reverse\n",
            ",\n",
            " or\n",
            " modify\n",
            " the\n",
            " decision\n",
            ".\n",
            "\n",
            "\n",
            "-\n",
            " If\n",
            " a\n",
            " party\n",
            " is\n",
            " still\n",
            " unsat\n",
            "isf\n",
            "ied\n",
            " after\n",
            " the\n",
            " appeals\n",
            " court\n",
            " rules\n",
            ",\n",
            " they\n",
            " can\n",
            " petition\n",
            " the\n",
            " Supreme\n",
            " Court\n",
            " to\n",
            " hear\n",
            " the\n",
            " case\n",
            " through\n",
            " a\n",
            " writ\n",
            " of\n",
            " cert\n",
            "ior\n",
            "ari\n",
            ".\n",
            " \n",
            "\n",
            "\n",
            "-\n",
            " The\n",
            " Supreme\n",
            " Court\n",
            " gets\n",
            " thousands\n",
            " of\n",
            " cert\n",
            " petitions\n",
            " every\n",
            " year\n",
            " but\n",
            " usually\n",
            " only\n",
            " agrees\n",
            " to\n",
            " hear\n",
            " about\n",
            " 100\n",
            "-\n",
            "150\n",
            " of\n",
            " cases\n",
            " that\n",
            " have\n",
            " significant\n",
            " national\n",
            " importance\n",
            " or\n",
            " where\n",
            " lower\n",
            " courts\n",
            " disagree\n",
            " on\n",
            " federal\n",
            " law\n",
            ".\n",
            " \n",
            "\n",
            "\n",
            "-\n",
            " If\n",
            " 4\n",
            " out\n",
            " of\n",
            " the\n",
            " 9\n",
            " Just\n",
            "ices\n",
            " vote\n",
            " to\n",
            " grant\n",
            " cert\n",
            " (\n",
            "agree\n",
            " to\n",
            " hear\n",
            " the\n",
            " case\n",
            "),\n",
            " it\n",
            " goes\n",
            " on\n",
            " the\n",
            " Supreme\n",
            " Court\n",
            "'s\n",
            " do\n",
            "cket\n",
            " for\n",
            " arguments\n",
            ".\n",
            "\n",
            "\n",
            "-\n",
            " The\n",
            " Supreme\n",
            " Court\n",
            " then\n",
            " hears\n",
            " oral\n",
            " arguments\n",
            ",\n",
            " considers\n",
            " written\n",
            " brief\n",
            "s\n",
            ",\n",
            " examines\n",
            " the\n",
            " lower\n",
            " court\n",
            " records\n",
            ",\n",
            " and\n",
            " issues\n",
            " a\n",
            " final\n",
            " ruling\n",
            " on\n",
            " the\n",
            " case\n",
            ",\n",
            " which\n",
            " serves\n",
            " as\n",
            " binding\n",
            " precedent\n"
          ]
        }
      ],
      "source": [
        "# @title Streaming Example: Request Claude-2\n",
        "messages = [\n",
        "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "  {\"role\": \"user\", \"content\": \"how does a court case get to the Supreme Court?\"}\n",
        "  ]\n",
        "\n",
        "result = litellm.completion('claude-2', messages, stream=True)\n",
        "for chunk in result:\n",
        "  print(chunk['choices'][0]['delta'])\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
